{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Conversation Chains and Memory with LCEL"
      ],
      "metadata": {
        "id": "TkZ-tYIsqsAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.2.0\n",
        "!pip install langchain-openai==0.1.7\n",
        "!pip install langchain-community==0.2.0\n",
        "!pip install langchain-chroma==0.1.1"
      ],
      "metadata": {
        "id": "2evPp14fy258"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "id": "cv3JzCEx_PAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Connection to LLM\n",
        "\n",
        "Here we create a connection to ChatGPT to use later in our chains"
      ],
      "metadata": {
        "id": "2oeckxFBcc0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
      ],
      "metadata": {
        "id": "vHa9LMOfcOCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with LangChain Chains\n",
        "\n",
        "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. Also running on multiple data points can be done easily with chains.\n",
        "\n",
        "Chain's are the legacy interface for \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains.\n",
        "\n",
        "Here we will be using LCEL chains exclusively"
      ],
      "metadata": {
        "id": "FD3EKYm8M8gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Problem with Simple LLM Chains\n",
        "\n",
        "Simple LLM Chains cannot keep a track of past conversation history"
      ],
      "metadata": {
        "id": "iJ4-yIpEa1lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_txt = \"\"\"{query}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_txt)\n",
        "\n",
        "# you can also write this as llm_chain = prompt | chatgpt\n",
        "llm_chain = (\n",
        "    prompt\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ],
      "metadata": {
        "id": "jsPXwBEYavEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_chain.invoke({'query': 'What are the first four colors of a rainbow?'})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "0_XdMXuca-pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_chain.invoke({'query': 'and the other 3?'})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "_FeBWLIPbGd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with LCEL\n",
        "\n",
        "LangChain Expression Language (LCEL) connects prompts, models, parsers and retrieval components using a `|` pipe operator.\n",
        "\n",
        "A conversation chain basically consists of user prompts, historical conversation memory and the LLM. The LLM uses the history memory to give more contextual answers to every new prompt or user query.\n",
        "\n",
        "Memory is very important for having a true conversation with LLMs. LangChain allows us to manage conversation memory using various constructs. The main ones we will cover include:\n",
        "\n",
        "- ConversationBufferMemory\n",
        "- ConversationBufferWindowMemory\n",
        "- ConversationSummaryMemory\n",
        "- VectorStoreRetrieverMemory\n",
        "- ChatMessageHistory\n",
        "- SQLChatMessageHistory"
      ],
      "metadata": {
        "id": "ng-9uxLActTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with ConversationBufferMemory\n",
        "\n",
        "This is the simplest version of in-memory storage of historical conversation messages. It is basically a buffer for storing conversation memory.\n",
        "\n",
        "Remember if you have a really long conversation, you might exceed the max token limit of the context window allowed for the LLM."
      ],
      "metadata": {
        "id": "CCZDMCXoO0Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "id": "pfB6RuGLWjDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get historical conversation messages from the memory\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "71wPNZ19Wqsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets create a function now which returns the list of messages from memory\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "get_memory_messages('What are the first four colors of a rainbow?')"
      ],
      "metadata": {
        "id": "Uay5-chDYCfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing out the function with a runnable lambda which will go into our chain\n",
        "# this returns the history but we also need to send our current query to the prompt\n",
        "RunnableLambda(get_memory_messages).invoke({'query': 'What are the first four colors of a rainbow?'})"
      ],
      "metadata": {
        "id": "r1UnntaxW3Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we use a runnable passthrough to pass our current query untouched\n",
        "# along with the history messages to the next step in the chain\n",
        "RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ).invoke({'query': 'What are the first four colors of a rainbow?'})"
      ],
      "metadata": {
        "id": "xhxKM_gWXa-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "nnfHCMuFWtn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What are the first four colors of a rainbow?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "response"
      ],
      "metadata": {
        "id": "aUHqbObhW8OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "id": "Zz3I1Kt3Zx2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "DfoKQ8yjZ6zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(query, {\"output\": response.content})"
      ],
      "metadata": {
        "id": "yiBksfTgZ9z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "M8D7hnKUaix_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'and the other 3?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "Ra8taD1VanUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "J4hJh9jgbnMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Explain AI in 2 bullet points'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "TFogFx9JbqFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Now do the same for Deep Learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "67wu0ct9buF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What have we discussed so far?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "qG1UoqoybxLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "KH-9mqvXb0mM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with ConversationBufferWindowMemory\n",
        "\n",
        "If you have a really long conversation, you might exceed the max token limit of the context window allowed for the LLM when using `ConversationBufferMemory` so `ConversationBufferWindowMemory` helps in just storing the last K conversations (one conversation piece is one user message and the corresponding AI message from the LLM) and thus helps you manage token limits and costs"
      ],
      "metadata": {
        "id": "z02POB29ePic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# stores last 2 sets of human-AI conversations\n",
        "memory = ConversationBufferWindowMemory(return_messages=True, k=2)\n",
        "\n",
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "uz6mUhHTga6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What are the first four colors of a rainbow?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "_IsESp4jtHND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'and the other 3?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "NAveTuGstPsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Explain AI in 2 bullet points'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "dTLfJ_1WtWwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Now do the same for Deep Learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "KcHC0uWJtYda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "oaxZzNrctb1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What have we discussed so far?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "SA_ILGM0thBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "7ROjso3qtjiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with ConversationSummaryMemory\n",
        "\n",
        "If you have a really long conversation or a lot of messages, you might exceed the max token limit of the context window allowed for the LLM when using `ConversationBufferMemory`\n",
        "\n",
        "`ConversationSummaryMemory` creates a summary of the conversation history over time. This can be useful for condensing information from the conversation messages over time.\n",
        "\n",
        "This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
      ],
      "metadata": {
        "id": "PEqgmHFqwLqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationSummaryMemory(return_messages=True, llm=chatgpt)\n",
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return memory.load_memory_variables(query)['history']\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages as a summary to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "zsvU8Wk44uBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Explain AI in 2 bullet points'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "5cBOUQAG5t1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Now do the same for Deep Learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "DJlLfPTx5xpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "La4RXy_s50Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What have we discussed so far?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "_EAa87s156RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversation Chains with VectorStoreRetrieverMemory\n",
        "\n",
        "`VectorStoreRetrieverMemory` stores historical conversation messages in a vector store and queries the top-K most \"relevant\" history messages every time it is called.\n",
        "\n",
        "This differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions but retrieves history based on embedding similarity to the current question or prompt.\n",
        "\n",
        "In this case, the \"docs\" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation."
      ],
      "metadata": {
        "id": "VI6HK_dw8GA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Connect to  Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ],
      "metadata": {
        "id": "M8nHAP7XOGOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "jzrIVI2NAHC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a Vector Database to store conversation history\n",
        "\n",
        "Here we use the Chroma vector DB and initialize an empty database collection to store conversation messages"
      ],
      "metadata": {
        "id": "T_ehzSNyHZCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create empty vector DB\n",
        "chroma_db = Chroma(collection_name='history_db',\n",
        "                   embedding_function=openai_embed_model)"
      ],
      "metadata": {
        "id": "1FZOGTTi8LhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "SYS_PROMPT = \"\"\"Act as a helpful assistant and give brief answers\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# load 2 most similar conversation messages from vector db history for each new message \\ prompt\n",
        "# this uses cosine embedding similarity to load the top 2 similar messgages to the input prompt \\ query\n",
        "retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                   search_kwargs={\"k\": 2})\n",
        "memory = VectorStoreRetrieverMemory(retriever=retriever, return_messages=True)\n",
        "\n",
        "# creating our conversation chain now\n",
        "def get_memory_messages(query):\n",
        "    return [memory.load_memory_variables(query)['history']]\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(get_memory_messages)\n",
        "    ) # sends current query (input by user at runtime) and history messages to next step\n",
        "      |\n",
        "    prompt # creates prompt using the previous two variables\n",
        "      |\n",
        "    chatgpt # generates response using the prompt from previous step\n",
        ")"
      ],
      "metadata": {
        "id": "IquTZdMN8nb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Tell me about AI'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "VlBEB4DIGW83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What about deep learning'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "hSeAY7pZBqlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'Tell me about the fastest animal in the world in 2 lines'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "rH-gVRxXBkQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What about the cheetah?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "VPkUBjZYEnrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for a new query around machine learning even if the most recent conversation messages have been about animals, it uses the vector databases to load the last 2 historical conversations which are closest to the current question in terms of semantic similarity"
      ],
      "metadata": {
        "id": "f-8cs1yFG8LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.load_memory_variables({'query': 'What about machine learning?'})['history'])"
      ],
      "metadata": {
        "id": "UkMDLlqdGmhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = {'query': 'What about machine learning?'}\n",
        "response = conversation_chain.invoke(query)\n",
        "memory.save_context(query, {\"output\": response.content}) # remember to save your current conversation in memory\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "1KZx6U4PGi9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-user Conversation Chains with ChatMessageHistory\n",
        "\n",
        "The concept of `ChatHistory` refers to a class in LangChain which can be used to wrap an arbitrary chain. This `ChatHistory` will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future interactions will then load those messages and pass them into the chain as part of the input.\n",
        "\n",
        "The beauty of `ChatMessageHistory` is that we can store separate conversation histories per user or session which is often the need for real-world chatbots which will be accessed by many users at the same time.\n",
        "\n",
        "We use a `get_session_history` function which is expected to take in a `session_id` and return a Message History object. Everything is stored in memory. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain"
      ],
      "metadata": {
        "id": "Bqb1PyNFPTwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# used to retrieve conversation history from memory\n",
        "# based on a specific user or session ID\n",
        "history_store = {}\n",
        "def get_session_history(session_id):\n",
        "    if session_id not in history_store:\n",
        "        history_store[session_id] = ChatMessageHistory()\n",
        "    return history_store[session_id]\n",
        "\n",
        "# prompt to load in history and current input from the user\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Act as a helpful AI Assistant\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{human_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# create a basic LLM Chain\n",
        "llm_chain = (prompt_template\n",
        "                |\n",
        "             chatgpt)\n",
        "\n",
        "# create a conversation chain which can load memory based on specific user or session id\n",
        "conv_chain = RunnableWithMessageHistory(\n",
        "    llm_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"human_input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# create a utility function to take in current user input prompt and their session ID\n",
        "# streams result live back to the user from the LLM\n",
        "def chat_with_llm(prompt: str, session_id: str):\n",
        "    for chunk in conv_chain.stream({\"human_input\": prompt},\n",
        "                                   {'configurable': { 'session_id': session_id}}):\n",
        "        print(chunk.content, end=\"\")"
      ],
      "metadata": {
        "id": "zNT2eLWjPWtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test conversation chain for user 1"
      ],
      "metadata": {
        "id": "LNg8YkwyvXPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bob123'\n",
        "prompt = \"Hi I am Bob, can you explain AI in 3 bullet points?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "-34zIxbmSSCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Now do the same for deep learning\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "trsi1b4vSwaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Discuss briefly what have we discussed so far is bullet points?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "4uwXCxqTSzBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test conversation chain for user 2"
      ],
      "metadata": {
        "id": "mbU0RS9WvaUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'james007'\n",
        "prompt = \"Hi can you explain what is an LLM in 2 bullet points?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "7opAjYZTTi_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Actually I meant in the context of AI?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "zWP5yyElULYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Summarize briefly what we have discussed so far?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "spIcYAS7Ud5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'bob123'\n",
        "prompt = \"Discuss briefly what have we discussed so far is bullet points?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "PDOoqws52yuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-user Window-based Conversation Chains with persistence - SQLChatMessageHistory\n",
        "\n",
        "The beauty of `SQLChatMessageHistory` is that we can store separate conversation histories per user or session which is often the need for real-world chatbots which will be accessed by many users at the same time. Instead of in-memory we can store it in a SQL database which can be used to store a lot of conversations.\n",
        "\n",
        "We use a `get_session_history` function which is expected to take in a `session_id` and return a Message History object. Everything is stored in a SQL database. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain\n",
        "\n",
        "We also use a `memory_buffer_window` function to only use the top-K last historical conversations before sending it to the LLM, basically our own implementation of `ConversationBufferWindowMemory`"
      ],
      "metadata": {
        "id": "fm6pZinBV4Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removes the memory database file - usually not needed\n",
        "# you can run this only when you want to remove all conversation histories\n",
        "!rm memory.db"
      ],
      "metadata": {
        "id": "tLdc_EEzWSKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# used to retrieve conversation history from database\n",
        "# based on a specific user or session ID\n",
        "def get_session_history_db(session_id):\n",
        "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n",
        "\n",
        "# prompt to load in history and current input from the user\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Act as a helpful AI Assistant\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{human_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# create a memory buffer window function to return the last K conversations\n",
        "def memory_buffer_window(messages, k=2):\n",
        "    return messages[-(k+1):]\n",
        "\n",
        "# create a basic LLM Chain which only sends the last K conversations per user\n",
        "llm_chain = (\n",
        "    RunnablePassthrough.assign(history=lambda x: memory_buffer_window(x[\"history\"]))\n",
        "      |\n",
        "    prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ],
      "metadata": {
        "id": "J-Qam16QUgkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a conversation chain which can load memory based on specific user or session id\n",
        "conv_chain = RunnableWithMessageHistory(\n",
        "    llm_chain,\n",
        "    get_session_history_db,\n",
        "    input_messages_key=\"human_input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# create a utility function to take in current user input prompt and their session ID\n",
        "# streams result live back to the user from the LLM\n",
        "def chat_with_llm(prompt: str, session_id: str):\n",
        "    for chunk in conv_chain.stream({\"human_input\": prompt},\n",
        "                                   {'configurable': { 'session_id': session_id}}):\n",
        "        print(chunk.content, end=\"\")"
      ],
      "metadata": {
        "id": "GKepk9DBrRNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test conversation chain for user 1"
      ],
      "metadata": {
        "id": "gkPyZbTnxBSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'jim001'\n",
        "prompt = \"Hi can you tell me which is the fastest animal?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "ebkhHCjfWl0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what about the slowest animal?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "cW6On1LasjYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what about the largest animal?\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "gMY_wEU5soCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what topics have we discussed, show briefly as bullet points\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "UTQPpY8RWysk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test conversation chain for user 2"
      ],
      "metadata": {
        "id": "qDLyOrajxD8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = 'john005'\n",
        "prompt = \"Explain AI in 3 bullets to a child\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "YBe4sodJW9UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Now do the same for Generative AI\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "ebV3Mt-vXG3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Now do the same for machine learning\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "1sTqJOawtRqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what topics have we discussed, show briefly as bullet points\"\n",
        "chat_with_llm(prompt, user_id)"
      ],
      "metadata": {
        "id": "Riyf6VhZXLEf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}