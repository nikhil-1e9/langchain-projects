{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Document Splitters and Chunkers in LangChain"
      ],
      "metadata": {
        "id": "oblslgZFAwj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, HuggingFace and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.2.0\n",
        "!pip install langchain-openai==0.1.7\n",
        "!pip install langchain-community==0.2.0"
      ],
      "metadata": {
        "id": "2evPp14fy258"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes 2 - 5 mins to install on Colab\n",
        "!pip install \"unstructured[all-docs]==0.14.0\""
      ],
      "metadata": {
        "id": "CB6lHzbz5a10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing `unstructured`above remember to restart your session when it shows you the following popup, if it doesn't go to `Runtime`and `Restart Session`\n",
        "\n",
        "![](https://i.imgur.com/UOBaotk.png)"
      ],
      "metadata": {
        "id": "ZTFImul36TRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install OCR dependencies for unstructured\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "NhEW-tOywUgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-text-splitters==0.2.0\n",
        "!pip install tiktoken==0.7.0\n",
        "!pip install spacy\n",
        "!pip install sentence-transformers==2.7.0"
      ],
      "metadata": {
        "id": "qOr-Lu31Kepl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Splitting and Chunking\n",
        "\n",
        "After loading documents into LangChain, you might need to transform them for optimal use in your application. One common transformation is splitting a long document into smaller segments to fit within your model's context window. LangChain provides several built-in document transformers to facilitate the splitting, combining, filtering, and manipulating of documents.\n",
        "\n",
        "#### Process of Document Splitting:\n",
        "1. **Splitting into Chunks:**\n",
        "   - Break down the text into small, semantically meaningful units (typically sentences).\n",
        "   \n",
        "2. **Combining Chunks:**\n",
        "   - Assemble these smaller units into larger chunks until they reach a predefined size. This size is determined by a specific measurement function.\n",
        "\n",
        "3. **Creating Overlapping Chunks:**\n",
        "   - Once the maximum size is reached, finalize the chunk as an independent text piece.\n",
        "   - Begin a new chunk, incorporating some overlap with the previous chunk to maintain textual context.\n",
        "\n",
        "This approach ensures that semantically related text pieces are kept together, which is crucial for maintaining the meaning and continuity of the document.\n"
      ],
      "metadata": {
        "id": "b4oHpnfwgxg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "\n",
        "The `RecursiveCharacterTextSplitter` is a versatile tool within LangChain for splitting text based on a list of characters. This splitter is designed to handle various requirements through adjustable parameters.\n",
        "\n",
        "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
        "\n",
        "#### Features and Parameters:\n",
        "\n",
        "- **Character List:** Utilizes a specified list of characters to determine where splits should occur.\n",
        "- **Chunk Size:** Allows you to set the size of each chunk, helping ensure that chunks are manageable and suit the context window of your model.\n",
        "- **Overlap:** Configurable overlap between consecutive chunks to maintain context continuity across chunks.\n",
        "\n",
        "This splitter is particularly useful for texts where precise control over the splitting criteria is needed, allowing for customized chunking strategies based on specific characters.\n"
      ],
      "metadata": {
        "id": "z5ZnXxY1s6Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "doc = \"\"\"Welcome to Green Valley, a small town nestled in the heart of the mountains. With its picturesque landscapes and vibrant community life, Green Valley has been a hidden gem for years. The main street is lined with an array of shops and cafes, each offering a unique taste of local flavor and culture.\n",
        "On a typical afternoon, the town square comes alive with the bustling sounds of locals and visitors mingling. Children play near the fountain, artists display their crafts, and an old man tells stories of days gone by. The aroma of freshly baked bread wafts from the bakery, drawing a steady stream of customers.\n",
        "Green Valley is not only known for its scenic beauty but also for its annual festivals. The most anticipated event is the Harvest Festival, celebrated with great enthusiasm. Locals prepare months in advance, cultivating crops and crafting goods for the occasion. The festival features a parade, various competitions, and a night market that lights up the town with vibrant colors and joyous energy.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "hACFJAzXiS-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc)"
      ],
      "metadata": {
        "id": "jMxFKmB2nySH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting with smaller chunk size (total characters) makes more paragraphs"
      ],
      "metadata": {
        "id": "4ax0fwyEg-WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators= [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=0,\n",
        ")"
      ],
      "metadata": {
        "id": "zaRR1bOVipeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_text(doc)\n",
        "print(len(texts)) # 5"
      ],
      "metadata": {
        "id": "9rAeZNtkit1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in texts:\n",
        "    print(text)\n",
        "    print(len(text))\n",
        "    print()"
      ],
      "metadata": {
        "id": "GpBIzJuuivQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting with larger chunk size (total characters) makes less paragraphs"
      ],
      "metadata": {
        "id": "U1-s8ZJvhRfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators= [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(doc)\n",
        "print(len(texts)) # 3"
      ],
      "metadata": {
        "id": "5CfzUwzPiw_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in texts:\n",
        "    print(text)\n",
        "    print(len(text))\n",
        "    print()"
      ],
      "metadata": {
        "id": "6jpVyL-LkYGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`chunk_overlap` helps to mitigate loss of information when context is divided between chunks especially for really small chunks"
      ],
      "metadata": {
        "id": "BPRCFEhnhXYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators= [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=100,\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(doc)\n",
        "print(len(texts)) # 5"
      ],
      "metadata": {
        "id": "Gr4piu2tr-zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in texts:\n",
        "    print(text)\n",
        "    print(len(text))\n",
        "    print()"
      ],
      "metadata": {
        "id": "uCeybNlvsA9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can create LangChain `Document` chunks with the `create_documents` function"
      ],
      "metadata": {
        "id": "3gnSBdBMhtko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators= [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        ")"
      ],
      "metadata": {
        "id": "gPfiBhhohqDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = text_splitter.create_documents([doc])\n",
        "docs"
      ],
      "metadata": {
        "id": "6KyUGtZgsakM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CharacterTextSplitter\n",
        "\n",
        "The `CharacterTextSplitter` is a straightforward tool in LangChain for dividing text based on a specified character. It's designed to be simple yet effective, providing essential controls for customizing how text is segmented.\n",
        "\n",
        "#### Key Features and Parameters:\n",
        "- **Split Character:** By default, it uses a empty string character (\"\") to split the text, but this can be customized to any character you specify.\n",
        "- **Chunk Size:** Allows you to define the length of each chunk in terms of the number of characters. This is useful for ensuring each piece of text is of a manageable size for processing.\n",
        "- **Overlap:** You can set the amount of overlap between consecutive chunks. This helps maintain context and continuity when text is split into separate parts.\n",
        "\n",
        "This method is the simplest among text splitting tools, focusing on character-based division and providing straightforward measures for chunk length and overlap.\n",
        "\n",
        "To obtain the string content directly, use `.split_text`.\n",
        "\n",
        "To create LangChain `Document` objects (e.g., for use in downstream tasks), use `.create_documents`.\n"
      ],
      "metadata": {
        "id": "1-aH3bTGt8-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=200,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "docs = text_splitter.create_documents([doc])\n",
        "docs"
      ],
      "metadata": {
        "id": "CIZux8d4sTDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Splitters\n",
        "\n",
        "`RecursiveCharacterTextSplitter` includes pre-built lists of separators that are useful for splitting text in a specific programming language."
      ],
      "metadata": {
        "id": "zOyopthBnPi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
        "\n",
        "python_code = \"\"\"\n",
        "def hello_world():\n",
        "    print(\"Hello, World!\")\n",
        "hello_world()\n",
        "\"\"\"\n",
        "\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
        ")\n",
        "python_docs = python_splitter.create_documents([python_code])\n",
        "python_docs"
      ],
      "metadata": {
        "id": "ZzshoSAhslGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markdown Splitters\n",
        "\n",
        "We might want to chunk a document based on the structure. For example, a markdown file is organized by headers. Creating chunks within specific header groups is an intuitive idea. To address this challenge, we can use MarkdownHeaderTextSplitter. This will split a markdown file by a specified set of headers.\n",
        "\n",
        "For example, if we want to split this markdown:\n",
        "\n",
        "```\n",
        "markdown_document = \"\"\"\n",
        "# Team Introductions\n",
        "\n",
        "## Management Team\n",
        "\n",
        "Hi, this is Jim, the CEO.  \n",
        "Hi, this is Joe, the CFO.\n",
        "\n",
        "## Development Team\n",
        "\n",
        "Hi, this is Molly, the Lead Developer.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "We can specify the headers to split on:\n",
        "\n",
        "```\n",
        "[(\"#\", \"Header 1\"),\n",
        " (\"##\", \"Header 2\")]\n",
        "```\n",
        "\n",
        "And content is grouped or split by common headers:\n",
        "\n",
        "```\n",
        "Document(page_content='Hi, this is Jim, the CEO.\\nHi, this is Joe, the CFO.',\n",
        "metadata={'Header 1': 'Team Introductions', 'Header 2': 'Management Team'})\n",
        "\n",
        "Document(page_content='Hi, this is Molly, the Lead Developer.',\n",
        "metadata={'Header 1': 'Team Introductions', 'Header 2': 'Development Team'})\n",
        "```"
      ],
      "metadata": {
        "id": "VHTef-iTncH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_document = \"\"\"\n",
        "# Team Introductions\n",
        "\n",
        "## Management Team\n",
        "Hi, this is Jim, the CEO.\n",
        "Hi, this is Joe, the CFO.\n",
        "\n",
        "## Development Team\n",
        "Hi, this is Molly, the Lead Developer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vThmLNqDn47j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
        "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
        "md_header_splits"
      ],
      "metadata": {
        "id": "x-cyLDjtnbaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, `MarkdownHeaderTextSplitter` strips headers being split on from the output chunk's content. This can be disabled by setting `strip_headers = False`."
      ],
      "metadata": {
        "id": "0lmKlTlMpXdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n",
        "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
        "md_header_splits"
      ],
      "metadata": {
        "id": "11bwTlE_n22N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer based Splitting\n",
        "\n",
        "Language models have a token limit. You should not exceed the token limit. When you split your text into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model. Let's look at how we can chunk documents using different tokenizers\n",
        "\n"
      ],
      "metadata": {
        "id": "fGAYh61uvNm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tiktoken splitters\n",
        "\n",
        "[`tiktoken`](https://github.com/openai/tiktoken) is a fast BPE tokenizer created by OpenAI.\n",
        "\n",
        "We can use tiktoken to estimate tokens used. It will probably be more accurate for the OpenAI models. We measure the `chunk_size`here based on the number of tokens typically and not the number of characters\n",
        "\n",
        "For Open AI models, roughly 1 token = 3\\4 words.\n",
        "\n",
        "Approx: 100 tokens ~= 75 words.\n",
        "\n"
      ],
      "metadata": {
        "id": "SP49p9ivvgT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load a [`TokenTextSplitter`](https://api.python.langchain.com/en/latest/base/langchain_text_splitters.base.TokenTextSplitter.html) splitter, which works with `tiktoken` directly and will ensure each split is smaller than chunk size in terms of the number of tokens."
      ],
      "metadata": {
        "id": "onDLEQb-wK8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = \"\"\"Welcome to Green Valley, a small town nestled in the heart of the mountains. With its picturesque landscapes and vibrant community life, Green Valley has been a hidden gem for years. The main street is lined with an array of shops and cafes, each offering a unique taste of local flavor and culture.\n",
        "On a typical afternoon, the town square comes alive with the bustling sounds of locals and visitors mingling. Children play near the fountain, artists display their crafts, and an old man tells stories of days gone by. The aroma of freshly baked bread wafts from the bakery, drawing a steady stream of customers.\n",
        "Green Valley is not only known for its scenic beauty but also for its annual festivals. The most anticipated event is the Harvest Festival, celebrated with great enthusiasm. Locals prepare months in advance, cultivating crops and crafting goods for the occasion. The festival features a parade, various competitions, and a night market that lights up the town with vibrant colors and joyous energy.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w-q_NkXrn27Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(model_name='gpt-3.5-turbo',\n",
        "                                  chunk_size=30,\n",
        "                                  chunk_overlap=10)\n",
        "\n",
        "docs = text_splitter.create_documents([doc])"
      ],
      "metadata": {
        "id": "qK9K_SfkrO1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "tDqpxx0AsHik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "4GYgYZ2_wYhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "for d in docs:\n",
        "  print('Words:', len(d.page_content.split(' ')),\n",
        "        'Tokens:', len(enc.encode(d.page_content)),\n",
        "        'Chunk:', d.page_content)"
      ],
      "metadata": {
        "id": "m4iWR-rUsfvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Larger chunk size in terms of number of words \\ tokens will create lesser chunks or paragraphs as usual"
      ],
      "metadata": {
        "id": "FAN_09Xbwql9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = TokenTextSplitter(model_name='gpt-3.5-turbo',\n",
        "                                  chunk_size=100,\n",
        "                                  chunk_overlap=30)\n",
        "\n",
        "docs = text_splitter.create_documents([doc])"
      ],
      "metadata": {
        "id": "NS0OdkHusJBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "3Btf5UwJwxp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "uoxtHgITttVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "for d in docs:\n",
        "  print('Words:', len(d.page_content.split(' ')),\n",
        "        'Tokens:', len(enc.encode(d.page_content)),\n",
        "        'Chunk:', d.page_content)"
      ],
      "metadata": {
        "id": "opmQFRrXt6N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement a hard constraint on the chunk size, we can use `RecursiveCharacterTextSplitter.from_tiktoken_encoder`, where each split will be recursively split if it has a larger size and it makes the chunks more meaningful"
      ],
      "metadata": {
        "id": "oaMOR7uIw7CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=30,\n",
        ")\n",
        "\n",
        "docs = text_splitter.create_documents([doc])"
      ],
      "metadata": {
        "id": "wJGWnTsXuL-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "YZBBh3oruaCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "0zTAu5ePucZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "for d in docs:\n",
        "  print('Words:', len(d.page_content.split(' ')),\n",
        "        'Tokens:', len(enc.encode(d.page_content)),\n",
        "        'Chunk:', d.page_content)"
      ],
      "metadata": {
        "id": "-tuY0LLxufoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### spaCy\n",
        "\n",
        "[spaCy](https://spacy.io/) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.\n",
        "\n",
        "LangChain implements splitters based on the [spaCy tokenizer](https://spacy.io/api/tokenizer)."
      ],
      "metadata": {
        "id": "1oWaOQCzzo4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import SpacyTextSplitter\n",
        "\n",
        "text_splitter = SpacyTextSplitter(chunk_size=500,\n",
        "                                  chunk_overlap=50)\n",
        "\n",
        "docs = text_splitter.create_documents([doc])"
      ],
      "metadata": {
        "id": "6jhbMwzGxfCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "mZS9M4__xrFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "qahyJ2Bixs6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in docs:\n",
        "  print('Words:', len(d.page_content.split(' ')),\n",
        "        'Characters:', len(d.page_content),\n",
        "        'Chunk:', d.page_content)"
      ],
      "metadata": {
        "id": "jXWvwAHfx0PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SentenceTransformers\n",
        "\n",
        "The [`SentenceTransformersTokenTextSplitter`](https://api.python.langchain.com/en/latest/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html) is a specialized text splitter for use with the `sentence-transformer` language models.\n",
        "\n",
        "The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use."
      ],
      "metadata": {
        "id": "WTyoeBIwz8S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
        "\n",
        "splitter = SentenceTransformersTokenTextSplitter(model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "                                                 tokens_per_chunk=100,\n",
        "                                                 chunk_overlap=30)"
      ],
      "metadata": {
        "id": "at39DZguycdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = splitter.create_documents([doc])"
      ],
      "metadata": {
        "id": "XVZ-el44y6ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "xYSP54J4y_jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "FrfQZEvZzB3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in docs:\n",
        "  print('Words:', len(d.page_content.split(' ')),\n",
        "        'Characters:', len(d.page_content),\n",
        "        'Chunk:', d.page_content)"
      ],
      "metadata": {
        "id": "4UUVCBjMzUH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section-based Splitting in Unstructured.io\n",
        "\n",
        "Chunking functions in `unstructured` use metadata and document elements detected with partition functions to split a document into smaller parts for uses cases such as Retrieval Augmented Generation (RAG).\n",
        "\n",
        "`unstructured` uses specific knowledge about each document format to partition the document into semantic units (document elements), we only need to resort to text-splitting when a single element exceeds the desired maximum chunk size. Except in that case, all chunks contain one or more whole elements, preserving the coherence of semantic units established during partitioning.\n",
        "\n",
        "- Chunking is performed on document elements. It is a separate step performed after partitioning, on the elements produced by partitioning. (Although it can be combined with partitioning in a single step.)\n",
        "\n",
        "- Chunking combines consecutive elements to form chunks as large as possible without exceeding the maximum chunk size.\n",
        "\n",
        "- A single element that by itself exceeds the maximum chunk size is divided into two or more chunks using text-splitting.\n",
        "\n",
        "- Chunking produces a sequence of `CompositeElement`, `Table`, or `TableChunk` elements. Each “chunk” is an instance of one of these three types."
      ],
      "metadata": {
        "id": "r3VUgrs90QeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking Options:\n",
        "\n",
        "The following options are available to tune chunking behaviors. These are keyword arguments that can be used in a partitioning or chunking function call. All these options have defaults and need only be specified when a non-default setting is required. Specific chunking strategies (such as “by-title”) may have additional options.\n",
        "\n",
        "- `max_characters`: (default=500) - the hard maximum size for a chunk. No chunk will exceed this number of characters. A single element that by itself exceeds this size will be divided into two or more chunks using text-splitting.\n",
        "\n",
        "- `new_after_n_chars`: (default=max_characters) - the “soft” maximum size for a chunk. A chunk that already exceeds this number of characters will not be extended, even if the next _element_ would fit without exceeding the specified hard maximum. This can be used in conjunction with `max_characters` to set a “preferred” size, like “I prefer chunks of around 1000 characters, but I’d rather have a chunk of 1500 (max_characters) than resort to text-splitting”. This would be specified with `(..., max_characters=1500, new_after_n_chars=1000)`.\n",
        "\n",
        "- `overlap`: (default=0) - only when using text-splitting to break up an oversized chunk, include this number of characters from the end of the prior chunk as a prefix on the next. This can mitigate the effect of splitting the semantic unit represented by the oversized element at an arbitrary position based on text length.\n",
        "\n",
        "- `combine_text_under_n_chars argument`: This defaults to the same value as `max_characters` such that sequential small section chunks are combined to maximally fill the chunking window to produce a logically larger chunk\n"
      ],
      "metadata": {
        "id": "AT0QjtnC1GZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are currently two chunking strategies, `basic` and `by_title`.\n",
        "\n",
        "The `basic` strategy combines sequential elements to maximally fill each chunk while respecting both the specified max_characters (hard-max) and new_after_n_chars (soft-max) option values.\n",
        "\n",
        "The `by_title` chunking strategy preserves section boundaries and optionally page boundaries as well. “Preserving” here means that a single chunk will never contain text that occurred in two different sections."
      ],
      "metadata": {
        "id": "YD3Pru_f8k4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O 'layoutparser_paper.pdf' 'http://arxiv.org/pdf/2103.15348.pdf'"
      ],
      "metadata": {
        "id": "CWW-VjoF8zcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "\n",
        "# takes 3-4 mins on Colab\n",
        "loader = UnstructuredPDFLoader('./layoutparser_paper.pdf',\n",
        "                               strategy='hi_res',\n",
        "                               extract_images_in_pdf=False,\n",
        "                               infer_table_structure=True,\n",
        "                               chunking_strategy=\"by_title\",\n",
        "                               max_characters=4000,\n",
        "                               new_after_n_chars=3800,\n",
        "                               combine_text_under_n_chars=2000,\n",
        "                               mode='elements')\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "vjXPHQ8BuMgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "v0a61iHT9EfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[doc.metadata['category'] for doc in data]"
      ],
      "metadata": {
        "id": "DI7iQHC3zWYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "id": "dd19Thv2zgAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0].page_content)"
      ],
      "metadata": {
        "id": "G5yfZFWf9p5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[1]"
      ],
      "metadata": {
        "id": "w2Bt2qDSsRkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[1].page_content)"
      ],
      "metadata": {
        "id": "PYLVr10Hzjd5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}