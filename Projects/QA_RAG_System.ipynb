{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Build a QA RAG System on Wikipedia Data"
      ],
      "metadata": {
        "id": "TkZ-tYIsqsAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.2.0\n",
        "!pip install langchain-openai==0.1.7\n",
        "!pip install langchain-community==0.2.0\n",
        "!pip install sentence-transformers==2.7.0"
      ],
      "metadata": {
        "id": "2evPp14fy258"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Chroma Vector DB and LangChain wrapper"
      ],
      "metadata": {
        "id": "TlfidBdQZRGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma"
      ],
      "metadata": {
        "id": "uZKQDgQURhmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "id": "cv3JzCEx_PAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ],
      "metadata": {
        "id": "M8nHAP7XOGOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "jzrIVI2NAHC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Databases\n",
        "\n",
        "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector database takes care of storing embedded data and performing vector search for you.\n",
        "\n",
        "### Chroma Vector DB\n",
        "\n",
        "[Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0."
      ],
      "metadata": {
        "id": "IO3_BzTbTBcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the wikipedia data"
      ],
      "metadata": {
        "id": "RA_-hzHbFeSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you can't download using the following code\n",
        "# go to https://drive.google.com/file/d/1oWBnoxBZ1Mpeond8XDUSO6J9oAjcRDyW download it\n",
        "# manually upload it on colab\n",
        "!gdown 1oWBnoxBZ1Mpeond8XDUSO6J9oAjcRDyW"
      ],
      "metadata": {
        "id": "RZFMYH-yFhWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "\n",
        "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
        "\n",
        "docs = []\n",
        "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        data = json.loads(line.strip())\n",
        "\n",
        "        #Add all paragraphs\n",
        "        #passages.extend(data['paragraphs'])\n",
        "\n",
        "        #Only add the first paragraph\n",
        "        docs.append({\n",
        "                        'metadata': {\n",
        "                                        'title': data.get('title'),\n",
        "                                        'article_id': data.get('id')\n",
        "                        },\n",
        "                        'data': ' '.join(data.get('paragraphs')[0:3]) # restrict data to first 3 paragraphs to run later modules faster\n",
        "        })\n",
        "\n",
        "print(\"Passages:\", len(docs))"
      ],
      "metadata": {
        "id": "WwLEBC4nF9ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We subset our data so we only use a subset of wikipedia documents to run things faster\n",
        "docs = [doc for doc in docs for x in ['linguistics', 'india', 'cheetah']\n",
        "              if x in doc['data'].lower().split()]"
      ],
      "metadata": {
        "id": "oABVR8m-6ZiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "G4E1zYFSG7J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:3]"
      ],
      "metadata": {
        "id": "aSbhERAyGw0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create LangChain Documents"
      ],
      "metadata": {
        "id": "yPjmN39-Nv1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = [Document(page_content=doc['data'],\n",
        "                 metadata=doc['metadata']) for doc in docs]"
      ],
      "metadata": {
        "id": "E9KThjCwN0Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:3]"
      ],
      "metadata": {
        "id": "cFMT5Y0xN_PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "ccGsE7JYOilo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split larger documents into smaller chunks"
      ],
      "metadata": {
        "id": "ZaSljqmlOFwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
        "chunked_docs = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "-f9rqPnROE82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_docs[:3]"
      ],
      "metadata": {
        "id": "cqqp0RdkOsrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunked_docs)"
      ],
      "metadata": {
        "id": "q-1rG2xnOqcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Vector DB and persist on disk\n",
        "\n",
        "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
      ],
      "metadata": {
        "id": "-PnV9lAXZw9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
        "chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "                                  collection_name='rag_wikipedia_db',\n",
        "                                  embedding=openai_embed_model,\n",
        "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./wikipedia_db\")"
      ],
      "metadata": {
        "id": "kRYfcrsHUxyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ju_zBIj1Zsb"
      },
      "source": [
        "### Load Vector DB from disk\n",
        "\n",
        "This is just to show once you have a vector database on disk you can just load and create a connection to it anytime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNvj0dDH1WDg"
      },
      "outputs": [],
      "source": [
        "# load from disk\n",
        "chroma_db = Chroma(persist_directory=\"./wikipedia_db\",\n",
        "                   collection_name='rag_wikipedia_db',\n",
        "                   embedding_function=openai_embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFC3uPqYop0a"
      },
      "outputs": [],
      "source": [
        "chroma_db"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Connection to LLM\n",
        "\n",
        "Here we create a connection to ChatGPT to use later in our chains"
      ],
      "metadata": {
        "id": "2oeckxFBcc0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
      ],
      "metadata": {
        "id": "vHa9LMOfcOCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chained Retrieval Pipeline\n",
        "\n",
        "This strategy uses a chain of multiple retrievers sequentially to get to the most relevant documents. The following is the flow\n",
        "\n",
        "Similarity Retrieval → Compression Filter → Reranker Model Retrieval\n",
        "\n",
        "![](https://i.imgur.com/KriNRDJ.gif)\n"
      ],
      "metadata": {
        "id": "9YK8BBMBAU9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain.retrievers.document_compressors import LLMChainFilter\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# Retriever 1 - simple cosine distance based retriever\n",
        "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity\",\n",
        "                                              search_kwargs={\"k\": 5})\n",
        "\n",
        "#  decides which of the initially retrieved documents to filter out and which ones to return\n",
        "_filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
        "# Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
        "compressor_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=_filter, base_retriever=similarity_retriever\n",
        ")\n",
        "\n",
        "# download an open-source reranker model - BAAI/bge-reranker-v2-m3\n",
        "reranker = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\n",
        "reranker_compressor = CrossEncoderReranker(model=reranker, top_n=3)\n",
        "# Retriever 3 - Uses a Reranker model to rerank retrieval results from the previous retriever\n",
        "final_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker_compressor, base_retriever=compressor_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "UgZ4eMFc2GgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the old capital of India?\"\n",
        "docs = final_retriever.invoke(query)\n",
        "docs"
      ],
      "metadata": {
        "id": "dFtoRwndBZuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the fastest animal?\"\n",
        "docs = final_retriever.invoke(query)\n",
        "docs"
      ],
      "metadata": {
        "id": "9nKGhybhZVQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a QA RAG Chain\n",
        "\n",
        "To build a RAG chain we need a prompt template which instructs the LLM to not answer questions beyond the scope of the retrieved context documents, there are various such prompts out there, we craft one ourselves below"
      ],
      "metadata": {
        "id": "Bfn4Z4Em55bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If no context is present or if you don't know the answer, just say that you don't know.\n",
        "            Do not make up the answer unless it is there in the provided context.\n",
        "            Give a detailed answer with regard to the question.\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Answer:\n",
        "         \"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)"
      ],
      "metadata": {
        "id": "8Pu5U9HNkl-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LCEL Syntax for QA RAG Chain - Recommended\n",
        "\n",
        "Here we show you how to create the RAG chain using LangChain's recommended LCEL"
      ],
      "metadata": {
        "id": "Ng4cKPYfmRyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (final_retriever\n",
        "                      |\n",
        "                    format_docs),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "      |\n",
        "    prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ],
      "metadata": {
        "id": "18p2bJahmLX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "query = \"What is the financial capital of India?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "W2Ion52G99-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the old capital of India?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "hdYPx4g39wpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about the fastest animal?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "Xb3nI-wM-K9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain linguistics in simple terms\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "vRCklZua-O7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who won the champions league in 2021\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "id": "mjqHaljE-Uh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}