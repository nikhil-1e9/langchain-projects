{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Explore QA RAG Chains with LCEL"
      ],
      "metadata": {
        "id": "jbw4wHV4zlKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.2.0\n",
        "!pip install langchain-openai==0.1.7\n",
        "!pip install langchain-community==0.2.0"
      ],
      "metadata": {
        "id": "2evPp14fy258"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Chroma Vector DB and LangChain wrapper"
      ],
      "metadata": {
        "id": "TlfidBdQZRGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma"
      ],
      "metadata": {
        "id": "uZKQDgQURhmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "id": "cv3JzCEx_PAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ],
      "metadata": {
        "id": "M8nHAP7XOGOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "jzrIVI2NAHC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Databases\n",
        "\n",
        "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector database takes care of storing embedded data and performing vector search for you.\n",
        "\n",
        "### Chroma Vector DB\n",
        "\n",
        "[Chroma](https://docs.trychroma.com/getting-started) is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0."
      ],
      "metadata": {
        "id": "IO3_BzTbTBcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Vector DB and persist on disk\n",
        "\n",
        "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
      ],
      "metadata": {
        "id": "-PnV9lAXZw9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        " 'Quantum mechanics describes the behavior of very small particles.',\n",
        " 'Photosynthesis is the process by which green plants make food using sunlight.',\n",
        " 'Artificial Intelligence aims to create machines that can think and learn.',\n",
        " 'The pyramids of Egypt are historical monuments that have stood for thousands of years.',\n",
        " 'New Delhi is the capital of India and the seat of all three branches of the Government of India.',\n",
        " 'Biology is the study of living organisms and their interactions with the environment.',\n",
        " 'Music therapy can aid in the mental well-being of individuals.',\n",
        " 'Mumbai is the financial capital and the most populous city of India. It is the financial, commercial, and entertainment capital of South Asia.',\n",
        " 'The Milky Way is just one of billions of galaxies in the universe.',\n",
        " 'Economic theories help understand the distribution of resources in society.',\n",
        " 'Kolkata is the de facto cultural capital of India and a historically and culturally significant city. Calcutta served as the de facto capital of India until 1911.',\n",
        " 'Yoga is an ancient practice that involves physical postures and meditation.'\n",
        "]"
      ],
      "metadata": {
        "id": "oABVR8m-6ZiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create vector DB of docs and embeddings - takes 1 min on Colab\n",
        "chroma_db = Chroma.from_texts(texts=docs, collection_name='db_docs',\n",
        "                              # need to set the distance function to cosine else it uses euclidean by default\n",
        "                              # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                              collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                              embedding=openai_embed_model)"
      ],
      "metadata": {
        "id": "kRYfcrsHUxyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup a Vector Database Retriever\n",
        "\n",
        "Here we use the following retrieval strategy:\n",
        "\n",
        "- Similarity with Threshold Retrieval\n"
      ],
      "metadata": {
        "id": "bprZC4S6TLfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity with Threshold Retrieval\n",
        "\n",
        "We use cosine similarity here and retrieve the top 3 similar documents based on the user input query and also introduce a cutoff to not return any documents which are below a certain similarity threshold"
      ],
      "metadata": {
        "id": "8foBD2xmCDYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_threshold_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                                        search_kwargs={\"k\": 3,\n",
        "                                                                       \"score_threshold\": 0.3})"
      ],
      "metadata": {
        "id": "6ROSNwqeCMRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the capital of India?\"\n",
        "top3_docs = similarity_threshold_retriever.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "id": "Nv93k_QpCZv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the old capital of India?\"\n",
        "top3_docs = similarity_threshold_retriever.invoke(query)\n",
        "top3_docs"
      ],
      "metadata": {
        "id": "fpu7Wzo6Cre9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a QA RAG Chain\n",
        "\n",
        "To build a RAG chain we need a prompt template which instructs the LLM to not answer questions beyond the scope of the retrieved context documents, there are various such prompts out there, we craft one ourselves below"
      ],
      "metadata": {
        "id": "Bfn4Z4Em55bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If no context is present or if you don't know the answer, just say that you don't know.\n",
        "            Do not make up the answer unless it is there in the provided context.\n",
        "            Keep the answer concise and to the point with regard to the question.\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Answer:\n",
        "         \"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)"
      ],
      "metadata": {
        "id": "8Pu5U9HNkl-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Connection to LLM\n",
        "\n",
        "Here we create a connection to ChatGPT to use later in our chains"
      ],
      "metadata": {
        "id": "2oeckxFBcc0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)"
      ],
      "metadata": {
        "id": "vHa9LMOfcOCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Legacy LangChain Syntax for QA RAG Chain\n",
        "\n",
        "Here we show you how you can create your own QA RAG Chain with legacy syntax which works well but not recommended as LangChain has been migrating to LCEL chains."
      ],
      "metadata": {
        "id": "RomJYx8HlRs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "N2nYSn9ojyS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_rag_chain = RetrievalQA.from_chain_type(llm=chatgpt,\n",
        "                                           chain_type=\"stuff\",\n",
        "                                           retriever=similarity_threshold_retriever,\n",
        "                                           chain_type_kwargs={\"prompt\": prompt_template})"
      ],
      "metadata": {
        "id": "3Rygnfd1jrg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the capital of India?\"\n",
        "qa_rag_chain.invoke(query)"
      ],
      "metadata": {
        "id": "CRDBw3P9lfhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about new delhi in detail\"\n",
        "qa_rag_chain.invoke(query)"
      ],
      "metadata": {
        "id": "nq-HGSmsEy4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about the financial capital of India\"\n",
        "qa_rag_chain.invoke(query)"
      ],
      "metadata": {
        "id": "iwp5mV3Gl6gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who was the winner of the champions league in 2020?\"\n",
        "qa_rag_chain.invoke(query)"
      ],
      "metadata": {
        "id": "cRPKBtiUqJgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LCEL Syntax for QA RAG Chain - Recommended\n",
        "\n",
        "Here we show you how to create the RAG chain using LangChain's recommended LCEL"
      ],
      "metadata": {
        "id": "Ng4cKPYfmRyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (similarity_threshold_retriever\n",
        "                      |\n",
        "                    format_docs),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "      |\n",
        "    prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        ")"
      ],
      "metadata": {
        "id": "18p2bJahmLX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the capital of India?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "njFlSt9Smznk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Tell me about the financial capital of India in detail\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "-OfsGdmhnn6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the fastest animal?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "JCDDyZJon0nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How do plants survive?\"\n",
        "result = qa_rag_chain.invoke(query)\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "1ZQDNlNcn4Z2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}